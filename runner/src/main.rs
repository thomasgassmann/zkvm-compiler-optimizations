mod bench;

use bench::bench_utils::{add_benchmarks_for, read_config_json};
use clap::{command, Parser, Subcommand};
use cpuprofiler::PROFILER;
use criterion::{profiler::Profiler, Criterion};
use runner::types::{Config, MeasurementType, ProgramId, ProverId};
use std::{
    fs,
    path::{Path, PathBuf},
    time::Duration,
};

#[derive(Subcommand, Clone)]
pub enum EvalSubcommand {
    Criterion(CriterionArgs),
    Run(RunArgs),
}

#[derive(Parser, Clone)]
#[command(about = "Evaluate the performance of a zkVM on a program.")]
pub struct EvalArgs {
    #[command(subcommand)]
    command: EvalSubcommand,
}

#[derive(Parser, Clone)]
pub struct CriterionArgs {
    #[arg(long)]
    program: Option<Vec<ProgramId>>,
    #[arg(long)]
    zkvm: Option<Vec<ProverId>>,
    #[arg(long)]
    measurement: Option<Vec<MeasurementType>>,
    #[arg(long)]
    profile: Option<Vec<String>>,
    #[arg(long = "profile-time")]
    profile_time: Option<u64>,
}

#[derive(Parser, Clone)]
pub struct RunArgs {
    #[arg(long)]
    program: ProgramId,
    #[arg(long)]
    zkvm: ProverId,
    #[arg(long)]
    elf: String,
}

struct Cpuprofiler;

impl Profiler for Cpuprofiler {
    fn start_profiling(&mut self, benchmark_id: &str, _benchmark_dir: &Path) {
        let absolute_path = fs::canonicalize(PathBuf::from("profiles")).unwrap();
        let path_name = absolute_path.join(PathBuf::from(format!("{}.profile", benchmark_id)));
        fs::create_dir_all(&path_name.parent().unwrap()).unwrap();
        println!("Profiling to: {:?}", path_name);
        PROFILER
            .lock()
            .unwrap()
            .start(path_name.to_str().unwrap())
            .unwrap();
    }

    fn stop_profiling(&mut self, _benchmark_id: &str, _benchmark_dir: &Path) {
        // Stop the profiler.
        PROFILER.lock().unwrap().stop().unwrap();
    }
}

fn run_criterion(args: CriterionArgs) {
    let config: Config = read_config_json();

    let c: &mut criterion::Criterion = &mut Criterion::default()
        .profile_time(if args.profile_time.is_some() {
            println!("Profiling for {} seconds", args.profile_time.unwrap());
            Some(Duration::from_secs(args.profile_time.unwrap()))
        } else {
            None
        })
        .with_profiler(Cpuprofiler)
        .sample_size(10);

    let programs = match args.program {
        Some(program) => program,
        None => config.programs.list,
    };
    let measurements = match args.measurement {
        Some(measurement) => measurement,
        None => config.measurements,
    };
    let zkvms = match args.zkvm {
        Some(zkvm) => zkvm,
        None => config.zkvms,
    };
    let profiles = match args.profile {
        Some(profile) => profile,
        None => config.profiles.keys().cloned().collect(),
    };

    for program in programs.iter() {
        for measurement in measurements.iter() {
            for prover in zkvms.iter() {
                for profile in profiles.iter() {
                    println!("Bench: {}-{}-{}-{}", program, prover, measurement, profile);
                }
            }
        }
    }

    for program in programs {
        for prover in zkvms.iter() {
            let mut group = c.benchmark_group(&format!("{}-{}", program, prover));
            for measurement in measurements.iter() {
                for profile in profiles.iter() {
                    add_benchmarks_for(&program, &prover, &mut group, &measurement, &profile);
                }
            }

            group.finish();
        }
    }

    c.final_summary();
}

fn run_runner(run_args: RunArgs) {
    let elf: Vec<u8> = fs::read(run_args.elf).unwrap();
    let res = match run_args.zkvm {
        ProverId::Risc0 => {
            runner::report_risc0::Risc0Evaluator::eval(&elf, &run_args.program)
        }
        ProverId::SP1 => {
            runner::report_sp1::SP1Evaluator::eval(&elf, &run_args.program)
        }
    };
    println!("{:?}", res);
}

fn main() {
    sp1_core_machine::utils::setup_logger();
    let args = EvalArgs::parse();

    match args.command {
        EvalSubcommand::Criterion(criterion_args) => run_criterion(criterion_args),
        EvalSubcommand::Run(run_args) => run_runner(run_args),
    }
}
